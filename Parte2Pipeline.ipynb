{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta parte se utilizará otro conjunto muy conocido de datos para aprendizaje, que consiste también de textos en inglés. Se trata de un conjunto de noticias de prensa de la Agencia Reuters, *Reuters-21578 *, https://kdd.ics.uci.edu/databases/reuters21578/reuters21578.html\n",
    "\n",
    "Las noticias han sido categorizadas a mano, con etiquetas de varios tipos, incluyendo *temas, lugares, organizaciones, personas* y otros criterios. Están en formato SGML, las etiquetas aparecen embebidas en el texto. Existe documentación en el archivo README de la distribución.\n",
    "\n",
    "Nos interesaremos en el tipo de etiqueta *temas (en inglés en los textos topics)*. El objetivo es nuevamente aprender los temas a partir de los textos de las noticias, con la diferencia de que este es un problema multi-etiqueta : cada noticia puede tener varios temas. En vez de resolver el problema multi-etiqueta inicial (con más de 100 tópicos distintos) les pedimos que lo transformen según las siguientes simplificaciones: considere solo los 3 temas más frecuentes, y transforme el problema multi-etiqueta en 3 problemas de clasificación binaria\n",
    "\n",
    "No se realiza para esta parte una especificación detallada, sino que les pedimos a Uds. que armen los pasos de una solución y definan el detalle del notebook para esta parte. Mínimamente se espera que procesen la entrada SGML, y encuentren algún modo de enfocar el problema multi-etiqueta en la versión simplificada. Se debe proponer algún modo de tratar el texto, con eventuales mejoras respecto a la vectorización de la parte 1. Se deben aplicar clasificadores y medir su performance, mínimamente con precision, recall y medida-F. Finalmente, se espera una discusión detallada de todo lo realizado y eventuales propuestas de mejora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Puntos a tener en cuenta del conjunto de datos:\n",
    "* En Topics dentro de la etiqueta <REUTERS> puede haber:\n",
    "    * YES: Tiene al menos una categoría(puede existir algún error y no tener ningúna categoria)\n",
    "    * NO: No tiene categoría(puede existir algún error y tener alguna categoria, supuestamente en esta versión no sucede)\n",
    "    * BYPASS: No está indexado, no se verificó la categoría\n",
    "* Hay etiquetas que dividen el conjunto en train y test\n",
    "* Etiquetas:\n",
    "    * <TOPICS>, </TOPICS> [ONCE, SAMELINE]: Lista de categorias. Si hay alguna estan delimitadas por <D></D>.\n",
    "    * <UNKNOWN>, </UNKNOWN> metadatos desconocidos\n",
    "    * <TEXT>, </TEXT> cuerpo del texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##1\n",
    "\n",
    "Se cargan los archivos a memoria, con la librería BeautifulSoup se parsea cada documento como xml dividiendo los documentos por noticias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "#Cargo los archivos como lista de strings en documentos\n",
    "lista_nombres_archivo = glob.glob('reuters21578/*.sgm')\n",
    "documentos = []\n",
    "for nombre_archivo in lista_nombres_archivo:\n",
    "    with open (nombre_archivo, \"r\") as archivo:\n",
    "        documento = archivo.read()\n",
    "        documentos.append(documento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from BeautifulSoup import BeautifulSoup\n",
    "\n",
    "raw_reuter_news = []\n",
    "for documento in documentos:\n",
    "    #coloco el contenido de los tags 'reuters' en otra lista: raw_reuter_news\n",
    "    #esa lista contendra las noticias separadas en formato raw\n",
    "    soup = BeautifulSoup(documento)\n",
    "    for raw_reuter in soup('reuters'):\n",
    "        raw_reuter_news.append(raw_reuter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#verifico cantidad\n",
    "print len(raw_reuter_news)\n",
    "assert(21578 == len(raw_reuter_news))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2\n",
    "\n",
    "Se cuentan la cantidad de noticias por tema y se consideran los temas más frecuentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_news_x_topics = {}\n",
    "for raw_reuter_soup in raw_reuter_news:\n",
    "    #para cada noticia\n",
    "    for topics_soup in raw_reuter_soup.topics:\n",
    "        #para cada topic en la lista de topics \n",
    "        #agrego la noticia a un diccionario clave topic y valor lista de noticias con ese topic.\n",
    "        for topic_soup in topics_soup:\n",
    "            try:\n",
    "                #ya existe la lista.\n",
    "                raw_news_x_topics[topic_soup].append(raw_reuter_soup)\n",
    "            except KeyError:\n",
    "                #es el primer valor\n",
    "                raw_news_x_topics[topic_soup] = [raw_reuter_soup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ordeno el diccionario por cantidad de noticias en cada topic.\n",
    "sorted_topics = sorted(raw_news_x_topics, key=lambda topic: len(raw_news_x_topics[topic]), reverse=True)\n",
    "#los tres topics mas usados\n",
    "primeros_topics = sorted_topics[:3]\n",
    "for topic in primeros_topics:\n",
    "        print topic,len(raw_news_x_topics[topic])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##3\n",
    "\n",
    "Se eliminan los metadatos y se separan los conjuntos de entrenamiento y test, para ésto se considera el atributo \"LEWISPLIT\". Se realizaran experimentos considerando por un lado noticias que tengan contenido más allá de la fecha y el titulo, sobre éstas noticias se experimentará tomando en cuenta sólo el contenido y tomando el contenido unido a la fecha y titulo. Por otra parte se considerarán todas las noticias aún si no tienen texto además de la fecha y titulo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ya no necesito los metadatos\n",
    "#separo en train y test segun el criterio lewissplit\n",
    "\n",
    "### PARA CUANDO BODY NO ES VACIO\n",
    "#solo body\n",
    "X_train = []\n",
    "X_test = []\n",
    "X_unused = []\n",
    "#info mas completa\n",
    "#body + fecha + titulo\n",
    "#incluye fecha y titulo\n",
    "X_train_plus = []\n",
    "X_test_plus = []\n",
    "X_unused_plus = []\n",
    "#para etiqueta EARN\n",
    "Y1_train = []\n",
    "Y1_test = []\n",
    "Y1_unused = []\n",
    "#para etiqueta ACQ\n",
    "Y2_train = []\n",
    "Y2_test = []\n",
    "Y2_unused = []\n",
    "#para etiqueta MONEY-FX\n",
    "Y3_train = []\n",
    "Y3_test = []\n",
    "Y3_unused = []\n",
    "\n",
    "##### PARA CUANDO BODY ES VACIO\n",
    "#incluye body o '' y fecha y titulo\n",
    "X_train_b = []\n",
    "X_test_b = []\n",
    "X_unused_b = []\n",
    "#para etiqueta EARN\n",
    "Y1_train_b = []\n",
    "Y1_test_b = []\n",
    "Y1_unused_b = []\n",
    "#para etiqueta ACQ\n",
    "Y2_train_b = []\n",
    "Y2_test_b = []\n",
    "Y2_unused_b = []\n",
    "#para etiqueta MONEY-FX\n",
    "Y3_train_b = []\n",
    "Y3_test_b = []\n",
    "Y3_unused_b = []\n",
    "\n",
    "#cuento los body vacios\n",
    "empty_body = 0\n",
    "empty_body_news = []\n",
    "\n",
    "for raw_reuter_soup in raw_reuter_news:\n",
    "    #para cada noticia\n",
    "    topics_de_la_noticia = []\n",
    "    for topics_soup in raw_reuter_soup.topics:\n",
    "        for topic_soup in topics_soup:\n",
    "            #para cada topic en la lista de topics \n",
    "            topics_de_la_noticia.append(topic_soup)\n",
    "    #veo si tiene las primeras categorias\n",
    "    es_earn = 1 if u'earn' in topics_de_la_noticia else 0\n",
    "    es_acq = 1 if u'acq' in topics_de_la_noticia else 0\n",
    "    es_money_fx = 1 if u'money-fx' in topics_de_la_noticia else 0\n",
    "    #extraigo contenido\n",
    "    body = raw_reuter_soup.find('body').text if raw_reuter_soup.find('body') is not None else None\n",
    "    title = raw_reuter_soup.find('title').text if raw_reuter_soup.find('title') is not None else None\n",
    "    date = raw_reuter_soup.find('dateline').text if raw_reuter_soup.find('dateline') is not None else None\n",
    "    ## BODY NO VACIO\n",
    "    if body is not None: \n",
    "        #conjunto entrenamiento\n",
    "        if raw_reuter_soup['lewissplit'] == 'TRAIN':\n",
    "            X_train.append(body)\n",
    "            X_train_plus.append('\\n'.join([title,body,date]))\n",
    "            Y1_train.append(es_earn)\n",
    "            Y2_train.append(es_acq)\n",
    "            Y3_train.append(es_money_fx)\n",
    "        #conjunto test\n",
    "        elif raw_reuter_soup['lewissplit'] == 'TEST':\n",
    "            X_test.append(body)\n",
    "            X_test_plus.append('\\n'.join([title,body,date]))\n",
    "            Y1_test.append(es_earn)\n",
    "            Y2_test.append(es_acq)\n",
    "            Y3_test.append(es_money_fx)\n",
    "        #otro\n",
    "        else:\n",
    "            X_unused.append(body)\n",
    "            X_unused_plus.append('\\n'.join([title,body,date]))\n",
    "            Y1_unused.append(es_earn)\n",
    "            Y2_unused.append(es_acq)\n",
    "            Y3_unused.append(es_money_fx)\n",
    "    ## BODY VACIO\n",
    "    else: \n",
    "        empty_body = empty_body + 1\n",
    "        empty_body_news.append(raw_reuter_soup)\n",
    "    ## SIEMPRE\n",
    "    #conjunto entrenamiento\n",
    "    if raw_reuter_soup['lewissplit'] == 'TRAIN':\n",
    "        X_train_b.append('\\n'.join(filter(None,[title,date])))\n",
    "        Y1_train_b.append(es_earn)\n",
    "        Y2_train_b.append(es_acq)\n",
    "        Y3_train_b.append(es_money_fx)\n",
    "    #conjunto test\n",
    "    elif raw_reuter_soup['lewissplit'] == 'TEST':\n",
    "        X_test_b.append('\\n'.join(filter(None,[title,date])))\n",
    "        Y1_test_b.append(es_earn)\n",
    "        Y2_test_b.append(es_acq)\n",
    "        Y3_test_b.append(es_money_fx)\n",
    "    #otro\n",
    "    else:\n",
    "        X_unused_b.append('\\n'.join(filter(None,[title,date])))\n",
    "        Y1_unused_b.append(es_earn)\n",
    "        Y2_unused_b.append(es_acq)\n",
    "        Y3_unused_b.append(es_money_fx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#verifico cantidad noticias\n",
    "news_n_topics = X_train + X_test + X_unused\n",
    "total = len(news_n_topics) + empty_body\n",
    "print \"Total: \", total\n",
    "assert(21578 == total)\n",
    "print\n",
    "print \"### Noticias con BODY: \",len(news_n_topics)\n",
    "#verifico cantidad earn\n",
    "print \"EARN: \",sum(Y1_train+Y1_test+Y1_unused)\n",
    "#verifico cantidad acq\n",
    "print \"ACQ: \",sum(Y2_train+Y2_test+Y2_unused)\n",
    "#verifico cantidad money-fx\n",
    "print \"MONEY-FX\", sum(Y3_train+Y3_test+Y3_unused)\n",
    "len_X_train = len(X_train)\n",
    "len_X_test = len(X_test)\n",
    "len_X_unused = len(X_unused)\n",
    "print\n",
    "print \"Train: \",len_X_train\n",
    "print \"Test: \",len_X_test\n",
    "print \"Unused: \",len_X_unused\n",
    "assert(len(news_n_topics)==len_X_train+len_X_test+len_X_unused)\n",
    "assert(len_X_train==len(Y1_train))\n",
    "assert(len_X_train==len(Y2_train))\n",
    "assert(len_X_train==len(Y3_train))\n",
    "assert(len_X_test==len(Y1_test))\n",
    "assert(len_X_test==len(Y2_test))\n",
    "assert(len_X_test==len(Y3_test))\n",
    "print\n",
    "news_n_topics_b = X_train_b + X_test_b + X_unused_b\n",
    "print \"### Noticias con o sin BODY: \",len(news_n_topics_b)\n",
    "#verifico cantidad earn\n",
    "print \"EARN: \",sum(Y1_train_b+Y1_test_b+Y1_unused_b)\n",
    "#verifico cantidad acq\n",
    "print \"ACQ: \",sum(Y2_train_b+Y2_test_b+Y2_unused_b)\n",
    "#verifico cantidad money-fx\n",
    "print \"MONEY-FX\", sum(Y3_train_b+Y3_test_b+Y3_unused_b)\n",
    "len_X_train_b = len(X_train_b)\n",
    "len_X_test_b = len(X_test_b)\n",
    "len_X_unused_b = len(X_unused_b)\n",
    "print\n",
    "print \"Train: \",len_X_train_b\n",
    "print \"Test: \",len_X_test_b\n",
    "print \"Unused: \",len_X_unused_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Organizacion de experimentos\n",
    "\n",
    "## NOTICIAS CON BODY\n",
    "Experimentos_A = [\n",
    "    (\"EARN\",X_train, X_test, Y1_train, Y1_test),\n",
    "    (\"ACQ\",X_train, X_test, Y2_train, Y2_test),\n",
    "    (\"MONEY-FX\",X_train, X_test, Y3_train, Y3_test)\n",
    "]\n",
    "\n",
    "## NOTICIAS CON BODY CONSIDERANDO FECHA Y TITULO\n",
    "Experimentos_B = [\n",
    "    (\"EARN\",X_train_plus, X_test_plus, Y1_train, Y1_test),\n",
    "    (\"ACQ\",X_train_plus, X_test_plus, Y2_train, Y2_test),\n",
    "    (\"MONEY-FX\",X_train_plus, X_test_plus, Y3_train, Y3_test)\n",
    "]\n",
    "\n",
    "## NOTICIAS CON O SIN BODY CONSIDERANDO FECHA Y TITULO\n",
    "Experimentos_C = [\n",
    "    (\"EARN\",X_train_b, X_test_b, Y1_train_b, Y1_test_b),\n",
    "    (\"ACQ\",X_train_b, X_test_b, Y2_train_b, Y2_test_b),\n",
    "    (\"MONEY-FX\",X_train_b, X_test_b, Y3_train_b, Y3_test_b)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Vectorizacion\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "count_vect2 = CountVectorizer(min_df=1)\n",
    "tidf_vect = TfidfVectorizer()\n",
    "hash_vect = HashingVectorizer(non_negative=True)\n",
    "Vectorizadores = [    \n",
    "    (\"TfidfVectorizer\", tidf_vect),\n",
    "    (\"CountVectorizer\", count_vect),\n",
    "    (\"CountVectorizer2\", count_vect2),\n",
    "    (\"HashingVectorizer\", hash_vect)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Comparación de diferentes clasificadores\n",
    "#Fuente: http://scikit-learn.org/stable/auto_examples/calibration/plot_compare_calibration.html\n",
    "\n",
    "# Creación de clasificadores\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\"\"\"\n",
    "lr = LogisticRegression()\n",
    "gnb = GaussianNB()\n",
    "svc = LinearSVC(C=1.0)\n",
    "rfc = RandomForestClassifier(n_estimators=100)\n",
    "\"\"\"\n",
    "multiNB = MultinomialNB()\n",
    "sgd = SGDClassifier()\n",
    "perceptron = Perceptron()\n",
    "passiveaggclass = PassiveAggressiveClassifier()\n",
    "passiveaggclass2 = PassiveAggressiveClassifier(loss='hinge',C=1.0)\n",
    "passiveaggclass3 = PassiveAggressiveClassifier(loss='squared_hinge',C=1.0)\n",
    "\n",
    "Clasificadores = [    \n",
    "    (\"MultinomialNB\", multiNB),\n",
    "    (\"SGD\", sgd),\n",
    "    (\"Perceptron\", perceptron),\n",
    "    (\"SGDClassifier\", sgd),\n",
    "    (\"Passive-Aggressive I\", passiveaggclass),\n",
    "    (\"Passive-Aggressive II\", passiveaggclass2),\n",
    "    (\"Passive-Aggressive III\", passiveaggclass3)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "resultados = []\n",
    "\n",
    "#Funcion que recibe \n",
    "#X conj entrenamiento\n",
    "#Y conj test\n",
    "#V conj vectorizadores\n",
    "#C conj clasificadores\n",
    "def run(train_data,train_target,test_data,test_target,V,C,matrix = True):\n",
    "    resultado = []\n",
    "    for nomvect, vectorizador in V:\n",
    "        #Vectorizo\n",
    "        X_train = vectorizador.fit_transform(train_data)        \n",
    "        X_test = vectorizador.transform(test_data)\n",
    "        f1_list = []\n",
    "        for nomclf,clasificador in C:\n",
    "            if (matrix):\n",
    "                print \"F1\"\n",
    "                print \"C: \" + nomclf + \", V: \" + nomvect   \n",
    "            if nomclf == 'RandomForest':\n",
    "                X_train = X_train.toarray()\n",
    "                X_test = X_test.toarray()\n",
    "            #Entreno\n",
    "            clasificador.fit(X_train, train_target)\n",
    "            #Predigo Test\n",
    "            prediccion = 0\n",
    "            prediccion = clasificador.predict(X_test)  \n",
    "            #Resultados\n",
    "            f1 = metrics.f1_score(test_target, prediccion, average='weighted')\n",
    "            f1_list.append(f1)\n",
    "            if (matrix):\n",
    "                print f1\n",
    "                print confusion_matrix(test_target,prediccion)\n",
    "        resultado.append(f1_list)\n",
    "    noms_vects, e = zip(*V)\n",
    "    noms_clsfs, e = zip(*C)\n",
    "    return (noms_vects,noms_clsfs,resultado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import cm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot(clasificadores, vectorizadores, medidaf):\n",
    "    \n",
    "    color=cm.rainbow(np.linspace(0,1,100))\n",
    "    indices = np.arange(len(clasificadores))\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    c = 1\n",
    "    contador = 0\n",
    "    i = 0\n",
    "    for vect in vectorizadores:\n",
    "        plt.barh(indices + i, medidaf[contador], .2, label=vect, color=color[c])\n",
    "        c=c+10\n",
    "        contador = contador + 1\n",
    "        i = i + .3\n",
    "    plt.yticks(())\n",
    "    plt.legend(loc='best')\n",
    "    plt.subplots_adjust(left=.25)\n",
    "    plt.subplots_adjust(top=.95)\n",
    "    plt.subplots_adjust(bottom=.05)\n",
    "\n",
    "    for i, c in zip(indices, clasificadores):\n",
    "        plt.text(-1.0, i, c)\n",
    "\n",
    "    plt.title(\"Medida-f por metodo y por clasificador.\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for nomcat,X_train,X_test,y_train,y_test in Experimentos_A:\n",
    "    print \"###### Categoria: \" + nomcat\n",
    "    print\n",
    "    noms_vects,nomclf,resultados = run(X_train,y_train,X_test,y_test,Vectorizadores,Clasificadores,False)\n",
    "    plot(nomclf, noms_vects, resultados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for nomcat,X_train,X_test,y_train,y_test in Experimentos_B:\n",
    "    print \"###### Categoria: \" + nomcat\n",
    "    print\n",
    "    run(X_train,y_train,X_test,y_test,Vectorizadores,Clasificadores,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for nomcat,X_train,X_test,y_train,y_test in Experimentos_C:\n",
    "    print \"###### Categoria: \" + nomcat\n",
    "    print\n",
    "    run(X_train,y_train,X_test,y_test,Vectorizadores,Clasificadores,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import cm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#clasificadores\n",
    "clf = [\"NB\", \"KNN\", \"ID3\", \"Red Neuronal\"]\n",
    "vectores = [\"CountVectorizer\", \"tf-idf\", \"HashingVectorizer\"]\n",
    "medidas = [[5, 3, 2, 3], [1, 5, 2, 5], [3, 2, 7, 2]]\n",
    "\n",
    "plot(clf, vectores, medidas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
