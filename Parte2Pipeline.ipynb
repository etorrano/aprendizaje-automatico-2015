{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta parte se utilizará otro conjunto muy conocido de datos para aprendizaje, que consiste también de textos en inglés. Se trata de un conjunto de noticias de prensa de la Agencia Reuters, *Reuters-21578 *, https://kdd.ics.uci.edu/databases/reuters21578/reuters21578.html\n",
    "\n",
    "Las noticias han sido categorizadas a mano, con etiquetas de varios tipos, incluyendo *temas, lugares, organizaciones, personas* y otros criterios. Están en formato SGML, las etiquetas aparecen embebidas en el texto. Existe documentación en el archivo README de la distribución.\n",
    "\n",
    "Nos interesaremos en el tipo de etiqueta *temas (en inglés en los textos topics)*. El objetivo es nuevamente aprender los temas a partir de los textos de las noticias, con la diferencia de que este es un problema multi-etiqueta : cada noticia puede tener varios temas. En vez de resolver el problema multi-etiqueta inicial (con más de 100 tópicos distintos) les pedimos que lo transformen según las siguientes simplificaciones: considere solo los 3 temas más frecuentes, y transforme el problema multi-etiqueta en 3 problemas de clasificación binaria\n",
    "\n",
    "No se realiza para esta parte una especificación detallada, sino que les pedimos a Uds. que armen los pasos de una solución y definan el detalle del notebook para esta parte. Mínimamente se espera que procesen la entrada SGML, y encuentren algún modo de enfocar el problema multi-etiqueta en la versión simplificada. Se debe proponer algún modo de tratar el texto, con eventuales mejoras respecto a la vectorización de la parte 1. Se deben aplicar clasificadores y medir su performance, mínimamente con precision, recall y medida-F. Finalmente, se espera una discusión detallada de todo lo realizado y eventuales propuestas de mejora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Puntos a tener en cuenta del conjunto de datos:\n",
    "* En Topics dentro de la etiqueta <REUTERS> puede haber:\n",
    "    * YES: Tiene al menos una categoría(puede existir algún error y no tener ningúna categoria)\n",
    "    * NO: No tiene categoría(puede existir algún error y tener alguna categoria, supuestamente en esta versión no sucede)\n",
    "    * BYPASS: No está indexado, no se verificó la categoría\n",
    "* Hay etiquetas que dividen el conjunto en train y test\n",
    "* Etiquetas:\n",
    "    * <TOPICS>, </TOPICS> [ONCE, SAMELINE]: Lista de categorias. Si hay alguna estan delimitadas por <D></D>.\n",
    "    * <UNKNOWN>, </UNKNOWN> metadatos desconocidos\n",
    "    * <TEXT>, </TEXT> cuerpo del texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "#Cargo los archivos como lista de strings en documentos\n",
    "lista_nombres_archivo = glob.glob('reuters21578/*.sgm')\n",
    "documentos = []\n",
    "for nombre_archivo in lista_nombres_archivo:\n",
    "    with open (nombre_archivo, \"r\") as archivo:\n",
    "        documento = archivo.read()\n",
    "        documentos.append(documento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print documentos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from BeautifulSoup import BeautifulSoup\n",
    "\n",
    "raw_reuter_news = []\n",
    "for documento in documentos:\n",
    "    #coloco el contenido de los tags 'reuters' en otra lista: raw_reuter_news\n",
    "    #esa lista contendra las noticias separadas en formato raw\n",
    "    soup = BeautifulSoup(documento)\n",
    "    for raw_reuter in soup('reuters'):\n",
    "        raw_reuter_news.append(raw_reuter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21578\n"
     ]
    }
   ],
   "source": [
    "#verifico cantidad\n",
    "print len(raw_reuter_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_news_x_topics = {}\n",
    "for raw_reuter_soup in raw_reuter_news:\n",
    "    #para cada noticia\n",
    "    for topics_soup in raw_reuter_soup.topics:\n",
    "        #para cada topic en la lista de topics \n",
    "        #agrego la noticia a un diccionario clave topic y valor lista de noticias con ese topic.\n",
    "        for topic_soup in topics_soup:\n",
    "            try:\n",
    "                #ya existe la lista.\n",
    "                raw_news_x_topics[topic_soup].append(raw_reuter_soup)\n",
    "            except KeyError:\n",
    "                #es el primer valor\n",
    "                raw_news_x_topics[topic_soup] = [raw_reuter_soup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "earn 3987\n",
      "acq 2448\n",
      "money-fx 801\n"
     ]
    }
   ],
   "source": [
    "#ordeno el diccionario por cantidad de noticias en cada topic.\n",
    "sorted_topics = sorted(raw_news_x_topics, key=lambda topic: len(raw_news_x_topics[topic]), reverse=True)\n",
    "#los tres topics mas usados\n",
    "primeros_topics = sorted_topics[:3]\n",
    "for topic in primeros_topics:\n",
    "        print topic,len(raw_news_x_topics[topic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ya no necesito los metadatos\n",
    "#separo en train y test segun el criterio lewissplit\n",
    "#solo body\n",
    "X_train = []\n",
    "X_test = []\n",
    "X_unused = []\n",
    "\n",
    "#aca dejo info mas completa por si es necesario\n",
    "#incluye fecha y titulo\n",
    "X_train_plus = []\n",
    "X_test_plus = []\n",
    "X_unused_plus = []\n",
    "\n",
    "#para etiqueta EARN\n",
    "Y1_train = []\n",
    "Y1_test = []\n",
    "Y1_unused = []\n",
    "\n",
    "#para etiquete ACQ\n",
    "Y2_train = []\n",
    "Y2_test = []\n",
    "Y2_unused = []\n",
    "\n",
    "\n",
    "#para etiqueta MONEY-FX\n",
    "Y3_train = []\n",
    "Y3_test = []\n",
    "Y3_unused = []\n",
    "\n",
    "for raw_reuter_soup in raw_reuter_news:\n",
    "    #para cada noticia\n",
    "    topics_de_la_noticia = []\n",
    "    for topics_soup in raw_reuter_soup.topics:\n",
    "        for topic_soup in topics_soup:\n",
    "            #para cada topic en la lista de topics \n",
    "            topics_de_la_noticia.append(topic_soup)\n",
    "    #veo si tiene las primeras categorias\n",
    "    es_earn = 1 if u'earn' in topics_de_la_noticia else 0\n",
    "    es_acq = 1 if u'acq' in topics_de_la_noticia else 0\n",
    "    es_money_fx = 1 if u'money-fx' in topics_de_la_noticia else 0\n",
    "    #extraigo contenido\n",
    "    body = raw_reuter_soup.find('body').text if raw_reuter_soup.find('body') is not None else None\n",
    "    title = raw_reuter_soup.find('title').text if raw_reuter_soup.find('title') is not None else None\n",
    "    date = raw_reuter_soup.find('dateline').text if raw_reuter_soup.find('dateline') is not None else None\n",
    "    #a veces no hay body\n",
    "    if body is not None: \n",
    "        #conjunto entrenamiento\n",
    "        if raw_reuter_soup['topics'] == 'YES':\n",
    "            if raw_reuter_soup['lewissplit'] == 'TRAIN':\n",
    "                X_train.append(body)\n",
    "                X_train_plus.append('\\n'.join([title,body,date]))\n",
    "                Y1_train.append(es_earn)\n",
    "                Y2_train.append(es_acq)\n",
    "                Y3_train.append(es_money_fx)\n",
    "            #conjunto test\n",
    "            elif raw_reuter_soup['lewissplit'] == 'TEST':\n",
    "                X_test.append(body)\n",
    "                X_test_plus.append('\\n'.join([title,body,date]))\n",
    "                Y1_test.append(es_earn)\n",
    "                Y2_test.append(es_acq)\n",
    "                Y3_test.append(es_money_fx)\n",
    "            #otro\n",
    "            else:\n",
    "                X_unused.append(body)\n",
    "                X_unused_plus.append('\\n'.join([title,body,date]))\n",
    "                Y1_unused.append(es_earn)\n",
    "                Y2_unused.append(es_acq)\n",
    "                Y3_unused.append(es_money_fx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total:  12344\n",
      "EARN:  3775\n",
      "ACQ:  2210\n",
      "MONEY-FX 682\n",
      "Train:  8762\n",
      "Test:  3009\n",
      "Unused:  573\n"
     ]
    }
   ],
   "source": [
    "#verifico cantidad noticias\n",
    "news_n_topics = X_train + X_test + X_unused\n",
    "print \"Total: \", len(news_n_topics)\n",
    "#verifico cantidad earn\n",
    "print \"EARN: \",sum(Y1_train+Y1_test+Y1_unused)\n",
    "#verifico cantidad acq\n",
    "print \"ACQ: \",sum(Y2_train+Y2_test+Y2_unused)\n",
    "#verifico cantidad money-fx\n",
    "print \"MONEY-FX\", sum(Y3_train+Y3_test+Y3_unused)\n",
    "\n",
    "len_X_train = len(X_train)\n",
    "len_X_test = len(X_test)\n",
    "len_X_unused = len(X_unused)\n",
    "\n",
    "print \"Train: \",len_X_train\n",
    "print \"Test: \",len_X_test\n",
    "print \"Unused: \",len_X_unused\n",
    "\n",
    "assert(len(news_n_topics)==len_X_train+len_X_test+len_X_unused)\n",
    "\n",
    "assert(len_X_train==len(Y1_train))\n",
    "assert(len_X_train==len(Y2_train))\n",
    "assert(len_X_train==len(Y3_train))\n",
    "\n",
    "assert(len_X_test==len(Y1_test))\n",
    "assert(len_X_test==len(Y2_test))\n",
    "assert(len_X_test==len(Y3_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Organizacion del conjunto de entrenamiento\n",
    "ConjuntoEntrenamiento = [\n",
    "    (\"EARN\",Y1_train, Y2_test),\n",
    "    (\"ACQ\",Y2_train, Y2_test),\n",
    "    (\"MONEY-FX\",Y3_train, Y3_test)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Vectorizacion\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "count_vect2 = CountVectorizer(min_df=1)\n",
    "tidf_vect = TfidfVectorizer()\n",
    "hash_vect = HashingVectorizer(non_negative=True)\n",
    "Vectorizadores = [\n",
    "    (\"CountVectorizer\", count_vect),\n",
    "    (\"CountVectorizer2\", count_vect2),\n",
    "    (\"TfidfVectorizer\", tidf_vect),\n",
    "    (\"HashingVectorizer\", hash_vect)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Comparación de diferentes clasificadores\n",
    "#Fuente: http://scikit-learn.org/stable/auto_examples/calibration/plot_compare_calibration.html\n",
    "\n",
    "# Creación de clasificadores\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\"\"\"\n",
    "lr = LogisticRegression()\n",
    "gnb = GaussianNB()\n",
    "svc = LinearSVC(C=1.0)\n",
    "rfc = RandomForestClassifier(n_estimators=100)\n",
    "\"\"\"\n",
    "multiNB = MultinomialNB()\n",
    "sgd = SGDClassifier()\n",
    "perceptron = Perceptron()\n",
    "passiveaggclass = PassiveAggressiveClassifier()\n",
    "passiveaggclass2 = PassiveAggressiveClassifier(loss='hinge',C=1.0)\n",
    "passiveaggclass3 = PassiveAggressiveClassifier(loss='squared_hinge',C=1.0)\n",
    "\n",
    "Clasificadores = [\n",
    "    (\"SGD\", sgd),\n",
    "    (\"MultinomialNB\", multiNB),\n",
    "    (\"Perceptron\", perceptron),\n",
    "    (\"SGDClassifier\", sgd),\n",
    "    (\"Passive-Aggressive I\", passiveaggclass),\n",
    "    (\"Passive-Aggressive II\", passiveaggclass2),\n",
    "    (\"Passive-Aggressive III\", passiveaggclass3)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrica F1 con los siguientes parametros\n",
      "Clasificador: SGD\n",
      "Vectorizador: CountVectorizer\n",
      "Categoria: EARN\n",
      "0.00243013365735\n",
      "Categoria: ACQ\n",
      "0.906095551895\n",
      "Categoria: MONEY-FX\n",
      "0.703125\n",
      "Metrica F1 con los siguientes parametros\n",
      "Clasificador: SGD\n",
      "Vectorizador: CountVectorizer2\n",
      "Categoria: EARN\n",
      "0.00243013365735\n",
      "Categoria: ACQ\n",
      "0.906095551895\n",
      "Categoria: MONEY-FX\n",
      "0.703125\n",
      "Metrica F1 con los siguientes parametros\n",
      "Clasificador: SGD\n",
      "Vectorizador: TfidfVectorizer\n",
      "Categoria: EARN\n",
      "0.00836820083682\n",
      "Categoria: ACQ\n",
      "0.931062449311\n",
      "Categoria: MONEY-FX\n",
      "0.743295019157\n",
      "Metrica F1 con los siguientes parametros\n",
      "Clasificador: SGD\n",
      "Vectorizador: HashingVectorizer\n",
      "Categoria: EARN"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "resultados = []\n",
    "\n",
    "for nomclf,clasificador in Clasificadores:\n",
    "    for nomvect, vectorizador in Vectorizadores:\n",
    "        print \"Metrica F1 con los siguientes parametros\"\n",
    "        print \"Clasificador: \" + nomclf\n",
    "        pipeline = Pipeline([\n",
    "              ('vectorizador', vectorizador),\n",
    "              ('clasificador', clasificador)\n",
    "            ])\n",
    "        \n",
    "        print \"Vectorizador: \" + nomvect \n",
    "        for nomcat,y_train,y_test in ConjuntoEntrenamiento:\n",
    "            #X_V_train = vectorizador.transform(X_train)\n",
    "            #Vectorizo test\n",
    "            #X_V_test = vectorizador.transform(X_test)\n",
    "            X_V_train = X_train\n",
    "            X_V_test = X_test\n",
    "            pipeline.fit(X_V_train, y_train)\n",
    "            prediccion = pipeline.predict(X_V_test)  \n",
    "            print \"Categoria: \" + nomcat\n",
    "            print metrics.f1_score(y_test, prediccion, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "X_V_train = vectorizer.fit_transform(X_train)\n",
    "X_V_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3009, 1048576)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Vectorizo test\n",
    "X_V_test = vectorizer.transform(X_test)\n",
    "X_V_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print Y1_train\n",
    "#print X_V_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EARN:  0.929851510497\n",
      "ACQ:  0.866375545852\n",
      "MONEY-FX:  0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf2 = SGDClassifier()\n",
    "clf3 = MultinomialNB()\n",
    "#entreno Y1\n",
    "clf.fit(X_V_train, Y1_train)\n",
    "#predigo test\n",
    "pred = clf.predict(X_V_test)\n",
    "print \"EARN: \",metrics.f1_score(Y1_test, pred, average='weighted')\n",
    "\n",
    "#entreno Y2\n",
    "clf2.fit(X_V_train, Y2_train)\n",
    "#predigo test\n",
    "pred2 = clf2.predict(X_V_test)\n",
    "print \"ACQ: \",metrics.f1_score(Y2_test, pred2, average='weighted')\n",
    "\n",
    "#entreno Y3\n",
    "clf3.fit(X_V_train, Y3_train)\n",
    "#predigo test\n",
    "pred3 = clf3.predict(X_V_test)\n",
    "print \"MONEY-FX: \",metrics.f1_score(Y3_test, pred3, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
