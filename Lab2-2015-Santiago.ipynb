{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos de la entrega"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Número de grupo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integrantes : Nombre y cédula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento de textos, parte 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Cada uno de los párrafos numerados requiere de la inserción a continuación de __al menos una__ celda, ya sea de tipo código o texto) *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizaremos procesamiento de textos sobre textos en inglés. El objetivo es determinar el tema del documento, se habla de **categorizar** el documento. Utilizaremos un conjunto de documentos, incluido en la distribución de sklearn, conocido con el nombre **20 Newsgroups** (qwone.com/~jason/20Newsgroups/ ). Se trata de un conjunto de aproximadamente 18.000 mensajes de correo, distribuidos en 20 temas distintos, tales como **religión cristiana**, **ateísmo**, **armas**, **mac hardware**, **computación gráfica**, etc.  Estos temas, a su vez, pueden agruparse entre sí con distintos grados de cercanía. El conjunto ha sido muy utilizado en todo tipo de experimentos de aprendizaje automático y se encuentran disponibles varias implementaciones.\n",
    "\n",
    "En esta tarea se pide que\n",
    "* Lea la documentación presente en la guía del usuario de scikit-learn, sección **5.7** y la documentación en el sitio web, previamente mencionado, sobre el conjunto de datos\n",
    "* Conteste algunas preguntas vinculadas\n",
    "* Realice categorización de subconjuntos del conjunto de datos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1  Inicialice su ambiente importando los módulos necesarios\n",
    "Cargue los datos de entrenamiento del dataset **20 Newsgroups**, con un módulo ya disponible en sklearn, \n",
    "seleccionando 5 categorías distintas, tales que 3 de ellas se encuentren separadas (temas no relacionados) según el esquema del sitio web de 20 Newsgroups y 3 de ellas en el mismo bloque(temas próximos).\n",
    "Despliegue las categorías de los documentos importados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorias elegidas:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['alt.atheism', 'comp.graphics', 'sci.electronics', 'sci.med', 'sci.space']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    ">>> cats = ['alt.atheism', 'sci.space', 'sci.electronics', 'sci.med', 'comp.graphics']\n",
    ">>> news = fetch_20newsgroups(subset='train', categories=cats)\n",
    "\n",
    "print('Categorias elegidas:')\n",
    ">>> list(news.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 ¿Cuántos archivos tiene su dataset?\n",
    "Imprima, del 4to archivo, el tema y las 30 primeras líneas.\n",
    "Imprima las categorías de los 1eros 20 documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de archivos del dataSet: 2842\n",
      "Informacion del cuarto archivo: sci.electronics\n",
      "\n",
      "From: km@ky3b.pgh.pa.us (Ken Mitchum)\n",
      "Subject: Re: How about a crash program in basic immunological research?\n",
      "Organization: KY3B - Vax Pittsburgh, PA\n",
      "Lines: 26\n",
      "\n",
      "In article <93099.141148C09630GK@wuvmd.wustl.edu>, C09630GK@WUVMD (Gary Kronk) writes:\n",
      "|> I have been contemplating this idea for some time as well. I am not a\n",
      "|> doctor, but my wife is a nurse and I know a lot of doctors and nurses.\n",
      "|> The point here being that doctors and nurses do not seem to get sick\n",
      "|> nearly as much as people outside the medical profession.\n",
      "\n",
      "This is a lovely area for anecdotes, but I am sure you are on to something.\n",
      "As a physician, I almost never get sick: usually, when something horrendous\n",
      "is going around, I either don't get it at all or get a very mild case.\n",
      "When I do get really sick, it is always something unusual.\n",
      "\n",
      "This was not the situation when I was in medical school, particularly on\n",
      "pediatrics. I never had younger siblings myself, and when I went on the\n",
      "pediatric wards I suddenly found myself confronting all sorts of infectious\n",
      "challenges that my body was not ready for. Pediatrics for me was three solid\n",
      "months of illness, and I had a temp of 104 when I took the final exam!\n",
      "\n",
      "I think what happens is that during training, and beyond, we are constantly\n",
      "exposed to new things, and we have the usual reactions to them, so that later\n",
      "on, when challenged with something, it is more likely a re-exposure for us,\n",
      "so we deal with it well and get a mild illness. I don't think it is that\n",
      "the immune system is hyped up in any way. Also, don't forget that the\n",
      "hospital flora is very different from the home, and we carry a lot of that\n",
      "around.\n",
      "\n",
      "-km\n",
      "\n",
      "\n",
      "Categorias de los primeros 20 articulos: \n",
      "comp.graphics\n",
      "comp.graphics\n",
      "alt.atheism\n",
      "sci.med\n",
      "sci.electronics\n",
      "comp.graphics\n",
      "alt.atheism\n",
      "sci.space\n",
      "comp.graphics\n",
      "comp.graphics\n",
      "sci.med\n",
      "comp.graphics\n",
      "comp.graphics\n",
      "alt.atheism\n",
      "sci.electronics\n",
      "comp.graphics\n",
      "sci.space\n",
      "sci.space\n",
      "comp.graphics\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cant = len(news.filenames)\n",
    "salida = 'Cantidad de archivos del dataSet: ' + str(cant)\n",
    "categoria4 = news.target_names[news.target[4]]\n",
    "salida = salida + \"\\n\" + 'Informacion del cuarto archivo: ' + str(categoria4) + \"\\n\" + \"\\n\" + news.data[3]\n",
    "salida = salida + \"\\n\" + \"\\n\" + 'Categorias de los primeros 20 articulos: ' + \"\\n\"\n",
    "for i in range(0, 19):\n",
    "    salida = salida + news.target_names[news.target[i]] + \"\\n\"\n",
    "\n",
    "print(salida)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 En cada documento, que es un correo electrónico, hay metadatos (quien envió el correo, fecha, organización, etc.). ¿Los metadatos pueden incidir en la categorización de un documento?\n",
    "¿Por qué puede ser conveniente sacarlos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los metadatos pueden llegar a aportar información para la categorización. Dado que los usuarios en general se especializan en pocas temáticas a la hora de realizar documentos, se podrían descartar algunas categorías solo con esto. La fecha y la organización no parecieran ser relevantes para la clasificación aunque no se puede descartar.\n",
    "\n",
    "Sin embargo, como los metadatos no forman parte del documento en sí, pueden también ser contraproducentes a la hora de evaluar su categoría. Conviene evaluar una instancia estrictamente por su información y no por la información secundaria que puede o no estar relacionada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 Genere una nueva versión del conjunto de datos, similar al ya generado, pero sin metadatos. Utilice para ello funcionalidades definidas en sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    ">>> from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    ">>> cats = ['alt.atheism', 'sci.space', 'sci.electronics', 'sci.med', 'comp.graphics']\n",
    ">>> metaless_news = fetch_20newsgroups(subset='train', categories=cats, remove=('headers', 'footers', 'quotes'))\n",
    ">>> print('Articulo 4 sin metadatos: ' + \"\\n\" + \"\\n\" + metaless_news.data[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 Un modo de utilizar los datos de texto en aprendizaje es pasar a un modelo vectorial. Explique brevemente como es el modelo vectorial con *bag-of-words*, qué hace *CountVectorizer* definido en sklearn, qué es *tf-idf* y cómo se implementa en Python. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo vectorial bag-of-words es un modelo para representar documentos de texto como vectores, cada elemento del vector representa una palabra presente en el documento y el valor de ese elemento es la cantidad de apariciones de su palabra en él. \n",
    "\n",
    "CountVectorizer es una forma de implmentar un bag-of-words. La operación toma un documento y devuelve una matriz Sparsa con una bag-of-words asociada a ese documento.\n",
    "\n",
    "El tf-idf es una medida para la importancia de una palabra en un documento perteneciente a un corpus mas grande de documentos. Se calcula como el producto de ortras dos medidas, el tf, que es la frecuencia con la que la palabra aparece en el documento, y el idf, que se calcula en base a la inversa de la cantidad de documentos del corpus en las que aparece la palabra. De esta forma se premian las palabras que aparecen varias veces, pero en pocos documentos.\n",
    "\n",
    "Para implementar tf-idf en Python podemos usar la función fetch_20newsgroups_vectorized, que en lugar de retornar los documentos retorna el tf-idf de cada una de sus palabras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 Repita el proceso de creación que aparece en el tutorial, llegando a un array \n",
    "que contiene el modelo vectorial para los datos actuales (los documentos seleccionados del dataset original)\n",
    "¿Cuál es el tamaño del vocabulario? ¿Cuál es el largo máximo de un documento? \n",
    "¿Qué conclusiones puede sacar sobre el array que representa a un documento?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups_vectorized\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "tfidf_news = fetch_20newsgroups_vectorized(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "salida = \"Vector tf-idf de un documento: \" + \"\\n\"\n",
    "salida = salida + str(tfidf_news.data[3]) + \"\\n\"\n",
    "\n",
    "count_vect = CountVectorizer()    \n",
    "token_news = count_vect.fit_transform(news.data)\n",
    "\n",
    "salida = salida + \"\\n\" + \"Tamanio del vocabulario: \"\n",
    "salida = salida + \"\\n\" + str(token_news.shape[1]) + \"\\n\"\n",
    "\n",
    "\n",
    "salida = salida + \"\\n\" + \"Maxima cantidad de palabras en un documento: \" + \"\\n\"\n",
    "\n",
    "print salida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7 Resuelva el problema de la categorización con algún algoritmo de aprendizaje (tratando de obtener buenos resultados). Calcule e imprima la medida-F, imprima la matriz de confusión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(news.data)\n",
    "\n",
    "clf = KNeighborsClassifier()\n",
    "clf.fit(token_news, news.target)\n",
    "\n",
    "count_vect2 = CountVectorizer()\n",
    "test_news = fetch_20newsgroups(subset='test', categories=cats, remove=('headers', 'footers', 'quotes'))\n",
    "token_test_news = vectorizer.transform(test_news.data)\n",
    "\n",
    "resultado = clf.predict(token_test_news)\n",
    "\n",
    "confusion = numpy.zeros((5, 5))\n",
    "\n",
    "print metrics.f1_score(test_news.target, resultado, average='weighted')\n",
    "print confusion_matrix(test_news.target,resultado)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8 Colapse los documentos con temas relacionados entre sí, asignádoles una única categoría con un nombre adecuado. Resuelva nuevamente el problema de la categorización, con el mismo algoritmo que en el paso anterior y los mismos cálculos de eficiencia. Compare y discuta los resultados. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento de textos, parte 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta parte se utilizará otro conjunto muy conocido de datos para aprendizaje, que consiste también de textos en inglés. Se trata de un conjunto de noticias de prensa de la Agencia Reuters, *Reuters-21578 *, https://kdd.ics.uci.edu/databases/reuters21578/reuters21578.html\n",
    "\n",
    "Las noticias han sido categorizadas a mano, con etiquetas de varios tipos, incluyendo *temas, lugares, organizaciones, personas* y otros criterios. Están en formato SGML, las etiquetas aparecen embebidas en el texto. Existe documentación en el archivo README de la distribución.\n",
    "\n",
    "Nos interesaremos en el tipo de etiqueta *temas (en inglés en los textos topics)*. El objetivo es nuevamente aprender los temas a partir de los textos de las noticias, con la diferencia de que este es un problema multi-etiqueta : cada noticia puede tener varios temas. En vez de resolver el problema multi-etiqueta inicial (con más de 100 tópicos distintos) les pedimos que lo transformen según las siguientes simplificaciones: considere solo los 3 temas más frecuentes, y transforme el problema multi-etiqueta en 3 problemas de clasificación binaria\n",
    "\n",
    "No se realiza para esta parte una especificación detallada, sino que les pedimos a Uds. que armen los pasos de una solución y definan el detalle del notebook para esta parte. Mínimamente se espera que procesen la entrada SGML, y encuentren algún modo de enfocar el problema multi-etiqueta en la versión simplificada. Se debe proponer algún modo de tratar el texto, con eventuales mejoras respecto a la vectorización de la parte 1. Se deben aplicar clasificadores y medir su performance, mínimamente con precision, recall y medida-F. Finalmente, se espera una discusión detallada de todo lo realizado y eventuales propuestas de mejora."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
