{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos de la entrega"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Número de grupo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integrantes : Nombre y cédula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento de textos, parte 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Cada uno de los párrafos numerados requiere de la inserción a continuación de __al menos una__ celda, ya sea de tipo código o texto) *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizaremos procesamiento de textos sobre textos en inglés. El objetivo es determinar el tema del documento, se habla de **categorizar** el documento. Utilizaremos un conjunto de documentos, incluido en la distribución de sklearn, conocido con el nombre **20 Newsgroups** (qwone.com/~jason/20Newsgroups/ ). Se trata de un conjunto de aproximadamente 18.000 mensajes de correo, distribuidos en 20 temas distintos, tales como **religión cristiana**, **ateísmo**, **armas**, **mac hardware**, **computación gráfica**, etc.  Estos temas, a su vez, pueden agruparse entre sí con distintos grados de cercanía. El conjunto ha sido muy utilizado en todo tipo de experimentos de aprendizaje automático y se encuentran disponibles varias implementaciones.\n",
    "\n",
    "En esta tarea se pide que\n",
    "* Lea la documentación presente en la guía del usuario de scikit-learn, sección **5.7** y la documentación en el sitio web, previamente mencionado, sobre el conjunto de datos\n",
    "* Conteste algunas preguntas vinculadas\n",
    "* Realice categorización de subconjuntos del conjunto de datos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1  Inicialice su ambiente importando los módulos necesarios\n",
    "Cargue los datos de entrenamiento del dataset **20 Newsgroups**, con un módulo ya disponible en sklearn, \n",
    "seleccionando 5 categorías distintas, tales que 3 de ellas se encuentren separadas (temas no relacionados) según el esquema del sitio web de 20 Newsgroups y 3 de ellas en el mismo bloque(temas próximos).\n",
    "Despliegue las categorías de los documentos importados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from pprint import pprint\n",
    "from datetime import datetime as dt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['misc.forsale',\n",
      " 'rec.motorcycles',\n",
      " 'sci.crypt',\n",
      " 'sci.electronics',\n",
      " 'sci.space']\n"
     ]
    }
   ],
   "source": [
    "cats = ['rec.motorcycles', 'misc.forsale', 'sci.space','sci.crypt','sci.electronics']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n",
    "pprint(list(news.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 ¿Cuántos archivos tiene su dataset?\n",
    "Imprima, del 4to archivo, el tema y las 30 primeras líneas.\n",
    "Imprima las categorías de los 1eros 20 documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2962"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cantidad de archivos en el conjunto de datos\n",
    "newsgroups_train.target.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sci.space\n"
     ]
    }
   ],
   "source": [
    "#Tema del 4to archivo\n",
    "print news.target_names[news.target[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: prb@access.digex.com (Pat)\n",
      "Subject: Re: Jemison on Star Trek\n",
      "Organization: Express Access Online Communications USA\n",
      "Lines: 14\n",
      "NNTP-Posting-Host: access.digex.net\n",
      "\n",
      "In article <1993Apr22.214735.22733@Princeton.EDU> phoenix.Princeton.EDU!carlosn (Carlos G. Niederstrasser) writes:\n",
      ">A transporter operator!?!?  That better be one important transport.  Usually  \n",
      ">it is a nameless ensign who does the job.  For such a guest appearance I would  \n",
      ">have expected a more visible/meaningful role.\n",
      "\n",
      "\n",
      "Christian  Slater, only gota  cameo on ST6,  \n",
      "\n",
      "and besides.\n",
      "\n",
      "Maybe she can't act:-)\n",
      "\n",
      "pat\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Primeras 30 líneas\n",
    "print(\"\\n\".join(news.data[3].split(\"\\n\")[:30]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sci.crypt\n",
      "sci.electronics\n",
      "sci.crypt\n",
      "sci.space\n",
      "misc.forsale\n",
      "sci.space\n",
      "sci.space\n",
      "sci.space\n",
      "sci.electronics\n",
      "misc.forsale\n",
      "sci.crypt\n",
      "sci.space\n",
      "sci.electronics\n",
      "sci.crypt\n",
      "sci.space\n",
      "sci.crypt\n",
      "sci.electronics\n",
      "rec.motorcycles\n",
      "misc.forsale\n",
      "misc.forsale\n"
     ]
    }
   ],
   "source": [
    "#Categorías de los primeros 30 documentos\n",
    "for t in news.target[:20]:\n",
    "    print(news.target_names[t])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 En cada documento, que es un correo electrónico, hay metadatos (quien envió el correo, fecha, organización, etc.). ¿Los metadatos pueden incidir en la categorización de un documento?\n",
    "¿Por qué puede ser conveniente sacarlos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los metadatos pueden influir en la categorización. Dado que los usuarios en general se especializan en pocas temáticas a la hora de realizar documentos, su email podria ser un atributo importante para descartar algunas categorías, también pueden influir negativamente: una dirección de email puede escribir habitualmente sobre un tema y esporádicamente sobre otro, sin embargo ésto no debería pesar en la categorización. Lo anterior es un caso de overfitting de los datos. Las etiquetas de los metadatos que aparecen siempre como \"Subject\" y \"From\" no influyen.\n",
    "\n",
    "Creemos que como los metadatos no forman parte del documento en sí, y pueden ser contraproducentes a la hora de evaluar su categoría, conviene evaluar una instancia estrictamente por su información y no por la información secundaria que puede o no estar relacionada, por lo tanto es conveniente eliminar los metadatos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 Genere una nueva versión del conjunto de datos, similar al ya generado, pero sin metadatos. Utilice para ello funcionalidades definidas en sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Christian  Slater, only gota  cameo on ST6,  \n",
      "\n",
      "and besides.\n",
      "\n",
      "Maybe she can't act:-)\n"
     ]
    }
   ],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=cats, remove=('headers', 'footers', 'quotes'))\n",
    "#4to documento sin metadatos\n",
    "print newsgroups_train.data[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 Un modo de utilizar los datos de texto en aprendizaje es pasar a un modelo vectorial. Explique brevemente como es el modelo vectorial con *bag-of-words*, qué hace *CountVectorizer* definido en sklearn, qué es *tf-idf* y cómo se implementa en Python. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La extracción de atributos consiste en transformar datos en general como texto o imágenes en atributos numéricos utilizables en métodos de aprendizaje automático. Cuando se trata de texto resulta imposible introducir una secuencia de simbolos de largo variable en los algoritmos de aprendizaje automático ya que la mayoría de ellos espera vectores de atributos númericos de tamaño fijo. La alternativa consiste en tokenizar los textos de forma de dividirlos en simbolos independientes llamados tokens utilizando los espacios y la puntuación, para posteriormente contar las ocurrencias de dichos tokens y normalizar el resultado. Cada token será considerado un atributo y la representación se denomina **Bag-Of-Words**. Dicha representación no considera el orden original de los tokens en el texto. **Count Vectorizer** es una implementación de extración de atributos de texto que realiza la tokenización y el conteo de las ocurrencias de cada token, **TfidfVectorizer** realiza además la normalización de dicho conteo: sucede que existen cierto tipo de palabras como los artículos que se repiten en la gran mayoría de los textos y que tienen poco significado real en el significado del texto, sin normalización el conteo de éste tipo de palabras podrían restar importancia de otras palabras menos frecuentes pero más significativas en los algoritmos utilizados.\n",
    "El **tf-idf** es una medida para la importancia de una palabra en un documento perteneciente a un corpus mas grande de documentos. Se calcula como el producto de ortras dos medidas, el **tf**, que es la frecuencia con la que la palabra aparece en el documento, y el **idf**, que se calcula en base a la inversa de la cantidad de documentos del corpus en las que aparece la palabra. De esta forma se premian las palabras que aparecen varias veces, pero en pocos documentos.\n",
    "\n",
    "Para implementar **TfidfVectorizer** se utiliza una función *fit_transform(..)* la cuál ajusta el estimador del método a los datos y transforma la matriz mencionada anteriormente en otra representación más adecuada al método."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 Repita el proceso de creación que aparece en el tutorial, llegando a un array \n",
    "que contiene el modelo vectorial para los datos actuales (los documentos seleccionados del dataset original)\n",
    "¿Cuál es el tamaño del vocabulario? ¿Cuál es el largo máximo de un documento? \n",
    "¿Qué conclusiones puede sacar sobre el array que representa a un documento?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31901\n"
     ]
    }
   ],
   "source": [
    "count_vect = CountVectorizer()    \n",
    "vec_vocab = count_vect.fit_transform(newsgroups_train.data)\n",
    "#Tamaño del vocabulario\n",
    "print str(vec_vocab.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "[ 37   0 128 ...,  30   3  12]\n",
      "11440\n"
     ]
    }
   ],
   "source": [
    "#convierto vocabulario en arreglo\n",
    "voc_arr = vec_vocab.toarray()\n",
    "#sumo cada fila para obtener la cantidad de palabras por documento\n",
    "dist = np.sum(voc_arr, axis=1)\n",
    "#imprimo sumatoria por cada documento\n",
    "print dist\n",
    "#Largo del documento con mayor cantidad de palabras\n",
    "print str(max(dist))\n",
    "\n",
    "\n",
    "\n",
    ":TODO VERIFICAR ESTO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7 Resuelva el problema de la categorización con algún algoritmo de aprendizaje (tratando de obtener buenos resultados). Calcule e imprima la medida-F, imprima la matriz de confusión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8 Colapse los documentos con temas relacionados entre sí, asignádoles una única categoría con un nombre adecuado. Resuelva nuevamente el problema de la categorización, con el mismo algoritmo que en el paso anterior y los mismos cálculos de eficiencia. Compare y discuta los resultados. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento de textos, parte 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta parte se utilizará otro conjunto muy conocido de datos para aprendizaje, que consiste también de textos en inglés. Se trata de un conjunto de noticias de prensa de la Agencia Reuters, *Reuters-21578 *, https://kdd.ics.uci.edu/databases/reuters21578/reuters21578.html\n",
    "\n",
    "Las noticias han sido categorizadas a mano, con etiquetas de varios tipos, incluyendo *temas, lugares, organizaciones, personas* y otros criterios. Están en formato SGML, las etiquetas aparecen embebidas en el texto. Existe documentación en el archivo README de la distribución.\n",
    "\n",
    "Nos interesaremos en el tipo de etiqueta *temas (en inglés en los textos topics)*. El objetivo es nuevamente aprender los temas a partir de los textos de las noticias, con la diferencia de que este es un problema multi-etiqueta : cada noticia puede tener varios temas. En vez de resolver el problema multi-etiqueta inicial (con más de 100 tópicos distintos) les pedimos que lo transformen según las siguientes simplificaciones: considere solo los 3 temas más frecuentes, y transforme el problema multi-etiqueta en 3 problemas de clasificación binaria\n",
    "\n",
    "No se realiza para esta parte una especificación detallada, sino que les pedimos a Uds. que armen los pasos de una solución y definan el detalle del notebook para esta parte. Mínimamente se espera que procesen la entrada SGML, y encuentren algún modo de enfocar el problema multi-etiqueta en la versión simplificada. Se debe proponer algún modo de tratar el texto, con eventuales mejoras respecto a la vectorización de la parte 1. Se deben aplicar clasificadores y medir su performance, mínimamente con precision, recall y medida-F. Finalmente, se espera una discusión detallada de todo lo realizado y eventuales propuestas de mejora."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
