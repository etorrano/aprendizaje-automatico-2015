{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Datos de la entrega"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Número de grupo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integrantes : Nombre y cédula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento de textos, parte 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Cada uno de los párrafos numerados requiere de la inserción a continuación de __al menos una__ celda, ya sea de tipo código o texto) *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizaremos procesamiento de textos sobre textos en inglés. El objetivo es determinar el tema del documento, se habla de **categorizar** el documento. Utilizaremos un conjunto de documentos, incluido en la distribución de sklearn, conocido con el nombre **20 Newsgroups** (qwone.com/~jason/20Newsgroups/ ). Se trata de un conjunto de aproximadamente 18.000 mensajes de correo, distribuidos en 20 temas distintos, tales como **religión cristiana**, **ateísmo**, **armas**, **mac hardware**, **computación gráfica**, etc.  Estos temas, a su vez, pueden agruparse entre sí con distintos grados de cercanía. El conjunto ha sido muy utilizado en todo tipo de experimentos de aprendizaje automático y se encuentran disponibles varias implementaciones.\n",
    "\n",
    "En esta tarea se pide que\n",
    "* Lea la documentación presente en la guía del usuario de scikit-learn, sección **5.7** y la documentación en el sitio web, previamente mencionado, sobre el conjunto de datos\n",
    "* Conteste algunas preguntas vinculadas\n",
    "* Realice categorización de subconjuntos del conjunto de datos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##1  \n",
    "\n",
    "Inicialice su ambiente importando los módulos necesarios\n",
    "Cargue los datos de entrenamiento del dataset **20 Newsgroups**, con un módulo ya disponible en sklearn, \n",
    "seleccionando 5 categorías distintas, tales que 3 de ellas se encuentren separadas (temas no relacionados) según el esquema del sitio web de 20 Newsgroups y 3 de ellas en el mismo bloque(temas próximos).\n",
    "Despliegue las categorías de los documentos importados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from pprint import pprint\n",
    "from datetime import datetime as dt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['misc.forsale',\n",
      " 'rec.motorcycles',\n",
      " 'sci.crypt',\n",
      " 'sci.electronics',\n",
      " 'sci.space']\n"
     ]
    }
   ],
   "source": [
    "cats = ['rec.motorcycles', 'misc.forsale', 'sci.space','sci.crypt','sci.electronics']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n",
    "pprint(list(newsgroups_train.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2\n",
    "\n",
    "¿Cuántos archivos tiene su dataset?\n",
    "Imprima, del 4to archivo, el tema y las 30 primeras líneas.\n",
    "Imprima las categorías de los 1eros 20 documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2962"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cantidad de archivos en el conjunto de datos\n",
    "newsgroups_train.target.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sci.space\n"
     ]
    }
   ],
   "source": [
    "#Tema del 4to archivo\n",
    "print newsgroups_train.target_names[newsgroups_train.target[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: prb@access.digex.com (Pat)\n",
      "Subject: Re: Jemison on Star Trek\n",
      "Organization: Express Access Online Communications USA\n",
      "Lines: 14\n",
      "NNTP-Posting-Host: access.digex.net\n",
      "\n",
      "In article <1993Apr22.214735.22733@Princeton.EDU> phoenix.Princeton.EDU!carlosn (Carlos G. Niederstrasser) writes:\n",
      ">A transporter operator!?!?  That better be one important transport.  Usually  \n",
      ">it is a nameless ensign who does the job.  For such a guest appearance I would  \n",
      ">have expected a more visible/meaningful role.\n",
      "\n",
      "\n",
      "Christian  Slater, only gota  cameo on ST6,  \n",
      "\n",
      "and besides.\n",
      "\n",
      "Maybe she can't act:-)\n",
      "\n",
      "pat\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Primeras 30 líneas\n",
    "print(\"\\n\".join(newsgroups_train.data[3].split(\"\\n\")[:30]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sci.crypt\n",
      "sci.electronics\n",
      "sci.crypt\n",
      "sci.space\n",
      "misc.forsale\n",
      "sci.space\n",
      "sci.space\n",
      "sci.space\n",
      "sci.electronics\n",
      "misc.forsale\n",
      "sci.crypt\n",
      "sci.space\n",
      "sci.electronics\n",
      "sci.crypt\n",
      "sci.space\n",
      "sci.crypt\n",
      "sci.electronics\n",
      "rec.motorcycles\n",
      "misc.forsale\n",
      "misc.forsale\n"
     ]
    }
   ],
   "source": [
    "#Categorías de los primeros 20 documentos\n",
    "for t in newsgroups_train.target[:20]:\n",
    "    print(newsgroups_train.target_names[t])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##3 \n",
    "\n",
    "En cada documento, que es un correo electrónico, hay metadatos (quien envió el correo, fecha, organización, etc.). ¿Los metadatos pueden incidir en la categorización de un documento?\n",
    "¿Por qué puede ser conveniente sacarlos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los metadatos pueden influir en la categorización. Dado que los usuarios en general se especializan en pocas temáticas a la hora de realizar documentos, su email podria ser un atributo importante para descartar algunas categorías, también pueden influir negativamente: una dirección de email puede escribir habitualmente sobre un tema y esporádicamente sobre otro, sin embargo ésto no debería pesar en la categorización. Lo anterior es un caso de overfitting de los datos. Las etiquetas de los metadatos que aparecen siempre como \"Subject\" y \"From\" pueden influir en menor medida.\n",
    "\n",
    "Creemos que como los metadatos no forman parte del documento en sí y pueden ser contraproducentes a la hora de evaluar su categoría, conviene evaluar una instancia estrictamente por su información y no por la información secundaria que puede o no estar relacionada, por lo tanto es conveniente eliminar los metadatos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##4 \n",
    "\n",
    "Genere una nueva versión del conjunto de datos, similar al ya generado, pero sin metadatos. Utilice para ello funcionalidades definidas en sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Christian  Slater, only gota  cameo on ST6,  \n",
      "\n",
      "and besides.\n",
      "\n",
      "Maybe she can't act:-)\n"
     ]
    }
   ],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=cats, remove=('headers', 'footers', 'quotes'))\n",
    "#4to documento sin metadatos\n",
    "print newsgroups_train.data[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##5 \n",
    "\n",
    "Un modo de utilizar los datos de texto en aprendizaje es pasar a un modelo vectorial. Explique brevemente como es el modelo vectorial con *bag-of-words*, qué hace *CountVectorizer* definido en sklearn, qué es *tf-idf* y cómo se implementa en Python. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La extracción de atributos consiste en transformar datos en general como texto o imágenes en atributos numéricos utilizables en métodos de aprendizaje automático. Cuando se trata de texto resulta imposible introducir una secuencia de simbolos de largo variable en los algoritmos de aprendizaje automático ya que la mayoría de ellos espera vectores de atributos númericos de tamaño fijo. La alternativa consiste en tokenizar los textos de forma de dividirlos en simbolos independientes llamados tokens utilizando los espacios y la puntuación, para posteriormente contar las ocurrencias de dichos tokens y normalizar el resultado. Cada token será considerado un atributo y la representación se denomina **Bag-Of-Words**. Dicha representación no considera el orden original de los tokens en el texto. **Count Vectorizer** es una implementación de extración de atributos de texto que realiza la tokenización y el conteo de las ocurrencias de cada token, **TfidfVectorizer** realiza además la normalización de dicho conteo: sucede que existen cierto tipo de palabras como los artículos que se repiten en la gran mayoría de los textos y que tienen poco significado real en el significado del texto, sin normalización el conteo de éste tipo de palabras podrían restar importancia de otras palabras menos frecuentes pero más significativas en los algoritmos utilizados.\n",
    "El **tf-idf** es una medida para la importancia de una palabra en un documento perteneciente a un corpus mas grande de documentos. Se calcula como el producto de ortras dos medidas, el **tf**, que es la frecuencia con la que la palabra aparece en el documento, y el **idf**, que se calcula en base a la inversa de la cantidad de documentos del corpus en las que aparece la palabra. De esta forma se premian las palabras que aparecen varias veces, pero en pocos documentos.\n",
    "\n",
    "Para implementar **TfidfVectorizer** se utiliza una función *fit_transform(..)* la cuál ajusta el estimador del método a los datos y transforma la matriz mencionada anteriormente en otra representación más adecuada al método."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##6 \n",
    "\n",
    "Repita el proceso de creación que aparece en el tutorial, llegando a un array \n",
    "que contiene el modelo vectorial para los datos actuales (los documentos seleccionados del dataset original)\n",
    "¿Cuál es el tamaño del vocabulario? ¿Cuál es el largo máximo de un documento? \n",
    "¿Qué conclusiones puede sacar sobre el array que representa a un documento?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31901\n"
     ]
    }
   ],
   "source": [
    "count_vect = CountVectorizer()    \n",
    "vec_vocab = count_vect.fit_transform(newsgroups_train.data)\n",
    "#Tamaño del vocabulario\n",
    "print str(vec_vocab.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 37   0 128 ...,  30   3  12]\n",
      "11440\n"
     ]
    }
   ],
   "source": [
    "#convierto vocabulario en arreglo\n",
    "voc_arr = vec_vocab.toarray()\n",
    "#sumo cada fila para obtener la cantidad de palabras por documento\n",
    "dist = np.sum(voc_arr, axis=1)\n",
    "#imprimo sumatoria por cada documento\n",
    "print dist\n",
    "#Largo del documento con mayor cantidad de palabras\n",
    "print max(dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##7 \n",
    "\n",
    "Resuelva el problema de la categorización con algún algoritmo de aprendizaje (tratando de obtener buenos resultados). Calcule e imprima la medida-F, imprima la matriz de confusión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newsgroups_test = fetch_20newsgroups(subset='test',categories=cats,remove=('headers'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Vectorizacion\n",
    "count_vect = CountVectorizer()\n",
    "count_vect2 = CountVectorizer(min_df=1)\n",
    "tidf_vect = TfidfVectorizer()\n",
    "hash_vect = HashingVectorizer(non_negative=True)\n",
    "Vectorizadores = [    \n",
    "    (\"TfidfVectorizer\", tidf_vect),\n",
    "    (\"CountVectorizer\", count_vect),\n",
    "    (\"HashingVectorizer\", hash_vect)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Comparación de diferentes clasificadores\n",
    "#Fuente: http://scikit-learn.org/stable/auto_examples/calibration/plot_compare_calibration.html\n",
    "\n",
    "# Creación de clasificadores\n",
    "multiNB = MultinomialNB(alpha=.01)\n",
    "sgd = SGDClassifier()\n",
    "perceptron = Perceptron()\n",
    "passiveaggclass = PassiveAggressiveClassifier()\n",
    "passiveaggclass2 = PassiveAggressiveClassifier(loss='hinge',C=1.0)\n",
    "passiveaggclass3 = PassiveAggressiveClassifier(loss='squared_hinge',C=1.0)\n",
    "forest = RandomForestClassifier(n_estimators = 100) \n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "Clasificadores = [    \n",
    "    (\"KNeighbors\", knn),\n",
    "   # (\"RandomForest\", forest),\n",
    "    (\"MultinomialNB\", multiNB),\n",
    "    #(\"SGD\", sgd),\n",
    "    #(\"Perceptron\", perceptron)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "resultados = []\n",
    "\n",
    "#Funcion que recibe \n",
    "#X conj entrenamiento\n",
    "#Y conj test\n",
    "#V conj vectorizadores\n",
    "#C conj clasificadores\n",
    "def run(train_data,train_target,test_data,test_target,V,C,matrix = True):\n",
    "    for nomvect, vectorizador in V:\n",
    "        #Vectorizo\n",
    "        X_train = vectorizador.fit_transform(train_data)        \n",
    "        X_test = vectorizador.transform(test_data)\n",
    "        for nomclf,clasificador in C:\n",
    "            print \"Metrica F1 con los siguientes parametros\"\n",
    "            print \"Clasificador: \" + nomclf + \", Vectorizador: \" + nomvect   \n",
    "            if nomclf == 'RandomForest':\n",
    "                X_train = X_train.toarray()\n",
    "                X_test = X_test.toarray()\n",
    "            #Entreno\n",
    "            clasificador.fit(X_train, train_target)\n",
    "            #Predigo Test\n",
    "            prediccion = 0\n",
    "            prediccion = clasificador.predict(X_test)  \n",
    "            #Resultados\n",
    "            print metrics.f1_score(newsgroups_test.target, prediccion, average='weighted')\n",
    "            if (matrix):\n",
    "                print confusion_matrix(newsgroups_test.target,prediccion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrica F1 con los siguientes parametros\n",
      "Clasificador: KNeighbors, Vectorizador: TfidfVectorizer\n",
      "0.235169465248\n",
      "[[ 70 105  66  86  63]\n",
      " [ 47 125  64  95  67]\n",
      " [ 51 112  98  82  53]\n",
      " [ 51 100  70 107  65]\n",
      " [ 55 114  80  76  69]]\n",
      "Metrica F1 con los siguientes parametros\n",
      "Clasificador: MultinomialNB, Vectorizador: TfidfVectorizer\n",
      "0.90550855421\n",
      "[[346  20   2  12  10]\n",
      " [  4 376   2   9   7]\n",
      " [  4   1 372  10   9]\n",
      " [ 12  13  40 319   9]\n",
      " [  1   5   9   6 373]]\n",
      "Metrica F1 con los siguientes parametros\n",
      "Clasificador: KNeighbors, Vectorizador: CountVectorizer\n",
      "0.410092128624\n",
      "[[196  80  54  32  28]\n",
      " [ 19 268  44  30  37]\n",
      " [ 20  97 212  22  45]\n",
      " [ 45 113 116  69  50]\n",
      " [ 19 123 134  21  97]]\n",
      "Metrica F1 con los siguientes parametros\n",
      "Clasificador: MultinomialNB, Vectorizador: CountVectorizer\n",
      "0.907873386434\n",
      "[[346  19   2  10  13]\n",
      " [  4 386   2   2   4]\n",
      " [  2   5 375   6   8]\n",
      " [ 14  11  41 316  11]\n",
      " [  2   7  12   5 368]]\n",
      "Metrica F1 con los siguientes parametros\n",
      "Clasificador: KNeighbors, Vectorizador: HashingVectorizer\n",
      "0.348930224039\n",
      "[[ 79  87 108  61  55]\n",
      " [ 19 171 134  25  49]\n",
      " [ 14  36 302  22  22]\n",
      " [ 21  45 214  72  41]\n",
      " [ 15  30 229  13 107]]\n",
      "Metrica F1 con los siguientes parametros\n",
      "Clasificador: MultinomialNB, Vectorizador: HashingVectorizer\n",
      "0.901758647866\n",
      "[[352  13   3  12  10]\n",
      " [  4 371   5  10   8]\n",
      " [  2   0 379   8   7]\n",
      " [  8  14  59 302  10]\n",
      " [  1   2  11   5 375]]\n"
     ]
    }
   ],
   "source": [
    "#Evaluo resultados\n",
    "run(newsgroups_train.data,newsgroups_train.target,newsgroups_test.data,newsgroups_test.target,Vectorizadores,Clasificadores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##8 \n",
    "\n",
    "Colapse los documentos con temas relacionados entre sí, asignádoles una única categoría con un nombre adecuado. Resuelva nuevamente el problema de la categorización, con el mismo algoritmo que en el paso anterior y los mismos cálculos de eficiencia. Compare y discuta los resultados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "news_train = fetch_20newsgroups(subset='train',remove=('headers', 'footers', 'quotes'), categories=cats)\n",
    "news_test = fetch_20newsgroups(subset='test',remove=('headers', 'footers', 'quotes'), categories=cats)\n",
    "newsgroup = [news_train, news_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for news in newsgroup:\n",
    "    for t in news.target:\n",
    "        #print(news.target_names[t])\n",
    "        if(news.target_names[t] in [\"sci.crypt\",\"sci.electronics\",\"sci.space\"]):\n",
    "            news.target_names[t]=\"science\"\n",
    "            #print(news.target_names[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrica F1 con los siguientes parametros\n",
      "Clasificador: KNeighbors, Vectorizador: TfidfVectorizer\n",
      "0.236478107524\n",
      "[[ 70  98  87  80  55]\n",
      " [ 42 144  74  83  55]\n",
      " [ 58 104  86  84  64]\n",
      " [ 59 105  72  95  62]\n",
      " [ 36  98  80 102  78]]\n",
      "Metrica F1 con los siguientes parametros\n",
      "Clasificador: MultinomialNB, Vectorizador: TfidfVectorizer\n",
      "0.838706946736\n",
      "[[335  23   3  18  11]\n",
      " [  8 358   9  13  10]\n",
      " [  4  23 347  13   9]\n",
      " [ 21  30  52 274  16]\n",
      " [  0  23  20  10 341]]\n",
      "Metrica F1 con los siguientes parametros\n",
      "Clasificador: KNeighbors, Vectorizador: CountVectorizer\n",
      "0.342850407415\n",
      "[[171  98  57  38  26]\n",
      " [ 53 196  68  41  40]\n",
      " [ 34 105 178  32  47]\n",
      " [ 52 108 116  54  63]\n",
      " [ 26 115 120  34  99]]\n",
      "Metrica F1 con los siguientes parametros\n",
      "Clasificador: MultinomialNB, Vectorizador: CountVectorizer\n",
      "0.832685624481\n",
      "[[331  24   2  21  12]\n",
      " [  9 364   3  16   6]\n",
      " [  2  29 342  12  11]\n",
      " [ 15  31  50 278  19]\n",
      " [  0  34  20  13 327]]\n",
      "Metrica F1 con los siguientes parametros\n",
      "Clasificador: KNeighbors, Vectorizador: HashingVectorizer\n",
      "0.309487444667\n",
      "[[ 72 100  93  60  65]\n",
      " [ 27 161 108  45  57]\n",
      " [ 16  49 274  30  27]\n",
      " [ 30  70 192  58  43]\n",
      " [ 16  47 204  34  93]]\n",
      "Metrica F1 con los siguientes parametros\n",
      "Clasificador: MultinomialNB, Vectorizador: HashingVectorizer\n",
      "0.829049479079\n",
      "[[338  20   7  15  10]\n",
      " [  7 343  20  17  11]\n",
      " [  3  22 353  11   7]\n",
      " [ 16  26  69 266  16]\n",
      " [  0  24  28   7 335]]\n"
     ]
    }
   ],
   "source": [
    "#Evaluo resultados\n",
    "run(news_train.data,news_train.target,news_test.data,news_test.target,Vectorizadores,Clasificadores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento de textos, parte 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta parte se utilizará otro conjunto muy conocido de datos para aprendizaje, que consiste también de textos en inglés. Se trata de un conjunto de noticias de prensa de la Agencia Reuters, *Reuters-21578 *, https://kdd.ics.uci.edu/databases/reuters21578/reuters21578.html\n",
    "\n",
    "Las noticias han sido categorizadas a mano, con etiquetas de varios tipos, incluyendo *temas, lugares, organizaciones, personas* y otros criterios. Están en formato SGML, las etiquetas aparecen embebidas en el texto. Existe documentación en el archivo README de la distribución.\n",
    "\n",
    "Nos interesaremos en el tipo de etiqueta *temas (en inglés en los textos topics)*. El objetivo es nuevamente aprender los temas a partir de los textos de las noticias, con la diferencia de que este es un problema multi-etiqueta : cada noticia puede tener varios temas. En vez de resolver el problema multi-etiqueta inicial (con más de 100 tópicos distintos) les pedimos que lo transformen según las siguientes simplificaciones: considere solo los 3 temas más frecuentes, y transforme el problema multi-etiqueta en 3 problemas de clasificación binaria\n",
    "\n",
    "No se realiza para esta parte una especificación detallada, sino que les pedimos a Uds. que armen los pasos de una solución y definan el detalle del notebook para esta parte. Mínimamente se espera que procesen la entrada SGML, y encuentren algún modo de enfocar el problema multi-etiqueta en la versión simplificada. Se debe proponer algún modo de tratar el texto, con eventuales mejoras respecto a la vectorización de la parte 1. Se deben aplicar clasificadores y medir su performance, mínimamente con precision, recall y medida-F. Finalmente, se espera una discusión detallada de todo lo realizado y eventuales propuestas de mejora."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
